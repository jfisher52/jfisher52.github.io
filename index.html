<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Home - Jillian Fisher</title>
  <meta name="description" content="Academic webpage of Jillian Fisher">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="/">
  <link rel="shortcut icon" type ="image/x-icon" href="/favicon.ico">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <link rel="preconnect" href="https://player.vimeo.com">
  <link rel="preconnect" href="https://i.vimeocdn.com">
  <link rel="preconnect" href="https://f.vimeocdn.com">



<!-- Google Analytics (original) -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');

</script>

<!-- Global site tag (gtag.js) - Google Analytics 4 -->
<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', '');
</script>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','');</script>
<!-- End Google Tag Manager -->



</head>


  <body>

    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<nav class="navbar sticky-top navbar-expand-md navbar-dark bg-dark">
    <a class="navbar-brand" href="/">
     <img src="/favicon.ico" width="30" height="30" style="margin-right:5px" class="d-inline-block align-top" alt="">
      Jillian Fisher
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarColor02">
        <ul class="navbar-nav mr-auto">
        <ul class="navbar-nav">
          <li class="nav-item">
              <a class="nav-link" href="/">Home</a>
          </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/about">About</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/publications">Publications</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/talks">Talks</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/software">Software</a>
           </li> 
          
           <li class="nav-item">
            <a class="nav-link" href="/teaching">Teaching</a>
           </li> 
          
        </ul>
  </div>
</nav>



    <div class="container-fluid">
      <div class="row">
          <div id="homeid" class="col-sm-7 col-xs-12">
	<h3 id="welcome">Welcome!</h3>

<p>I am a Statistics + CSE PhD student at the University of Washington researching AI alignment, safety, and societal impact.</p>

<p>Advised by Yejin Choi in the Paul G. Allen School of Computer Science and Engineering and Thomas Richardson in the Statistics Department, my focus in on projects that leverage statistical tools to advance methods and insights in human-centric NLP challenges. Currently, my work centers on alignment, controllable generations, and impact of AI on society.</p>

  </div>

  <br/>




  <div id="newsid" class="col-sm-5 col-xs-12" >
	<div>

<div class="jumbotron">
   <center>
   <a href="/about"><img src="/images/bodyshot_aiedited.png" width="60%" style="block:inline; margin-left:auto; margin-right:auto; margin-bottom:5px;"/></a>
   <h4>Jillian Fisher</h4>
   <div style="margin-bottom:5px">
   <a href="mailto:jrfish@uw.edu" target="_blank"><i class="fa fa-envelope-square fa-3x"></i></a> 
    <a href="/cv/JillianFisher_CV.pdf" target="_blank"><i class="ai ai-cv-square ai-3x"></i></a> 
    <a href="https://scholar.google.com/citations?user=Gnk0E_QAAAAJ&hl=en" target="_blank"><i class="ai ai-google-scholar-square ai-3x"></i></a> 
    <a href="https://github.com/jfisher52" target="_blank"><i class="fa fa-github-square fa-3x"></i></a> 
   
  </div>
  </center>

  <ul style="overflow: hidden">
  
  <!-- <li> (current) Ph.D. Statistics, University of Washington </li> -->
  <!-- <li> (2019) M.S. Statistics, Texas A&M </li>
  <li> (2015) B.A. Mathematics and Psychology, University of Texas </li> -->
  
  </ul>
</div>

</div>
</div>

<!-- PREPRINTS -->

<!-- <div class="jumbotron">
  <h3 id="preprints">Preprints</h3>
  <ol class="bibliography" reversed="reversed"><li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style> -->

<!-- <div class="text-justify"><span id="pn"> <b>Fisher, J.</b>, Appel, R., Young Park, C., Potter, Y.,Jiang, L., Sorensen, T., Feng, S., Tsvetkov, Y., Roberts, M., Pan, J., Song, D.,  &amp; Choi, Y. (2025) . <i>Political Neutrality in AI is Impossible- But Here is How to Approximate it. </i>Arxiv Coming Soon!</span></div>
<a href="/papers/political_neutrality_2025.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexpn()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractpn()">ABSTRACT</button>

<div id="apn" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{fisher_political_neutrality",
  title = {Political Neutrality in AI is Impossible- But Here is How to Approximate it},
  author = {Jillian Fisher, Ruth E. Appel, Chan Young Park, Yujin Potter, Liwei Jiang, Taylor Sorensen, Shangbin Feng, Yulia Tsvetkov, Margaret E. Roberts, Jennifer Pan, Dawn Song, Yejin Choi},
  year = {2025},
  note = {},
  file = {political_neutrality_2025.pdf}
}
</pre>
</div>

<div id="bpn" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>AI systems often exhibit political bias, influencing users’ opinions and decision-making. While political neutrality—defined as the absence of bias—is often seen as an ideal solution for fairness and safety, this position paper argues that true political neutrality is neither feasible nor universally desirable due to its subjective nature and the biases inherent in AI training data, algorithms, and user interactions. However, inspired by Joseph Raz’s philosophical insight that “neutrality [...] can be a matter of degree” (Raz, 1986), we argue that striving for some neutrality remains essential for promoting balanced AI interactions and mitigating user manipulation. Therefore, we use the term “approximation” of political neutrality to shift the focus from unattainable absolutes to achievable, practical proxies. We propose eight techniques for approximating neutrality across three levels of conceptualizing AI, examining their trade-offs and implementation strategies. In addition, we explore two concrete applications of these approximations to illustrate their practicality. Finally, we assess our framework on current large language models (LLMs) at the output level, providing a demonstration of how it can be evaluated. This work seeks to advance nuanced discussions of political neutrality in AI and promote the development of responsible, aligned language models.</pre>
</div>


<script>
function toggleBibtexpn(parameter) {
    var x= document.getElementById('apn');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractpn(parameter) {
    var x= document.getElementById('bpn');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style> -->


<!-- <div class="text-justify"><span id="bias_llm"> <b>Fisher, J.</b>, Feng, S., Aron, R., Richardson, T., Choi Y., Fisher, D., Pan J., Tsvetkov, Y.,  &amp; Reinecke, K (2024) . <i>Biased AI can Influence Political Decision-Making</i>. arxiv.org/abs/2410.06415.</span></div>
<a href="/papers/bias_llm.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexbias_llm()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractbias_llm()">ABSTRACT</button>

<div id="abias_llm" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{bias_llm",
  title = {Biased AI can Influence Political Decision-Making},
  author = {Jillian Fisher, Shangbin Feng, Robert Aron, Thomas Richardson, Yejin Choi, Daniel W. Fisher, Jennifer Pan, Yulia Tsvetkov, Katharina Reinecke},
  year = {2023},
  note = {https://arxiv.org/abs/2410.06415},
  file = {bias_llm.pdf}
}
</pre>
</div>

<div id="bbias_llm" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>As modern AI models become integral to everyday tasks, concerns about their inherent biases and their potential impact on human decision-making have emerged. While bias in models are well-documented, less is known about how these biases influence human decisions. This paper presents two interactive experiments investigating the effects of partisan bias in AI language models on political decision-making. Participants interacted freely with either a biased liberal, conservative, or unbiased control model while completing political decision-making tasks. We found that participants exposed to politically biased models were significantly more likely to adopt opinions and make decisions aligning with the AI’s bias, regardless of their personal political partisanship. However, we also discovered that prior knowledge about AI could lessen the impact of the bias, highlighting the possible importance of AI education for robust bias mitigation. Our findings not only highlight the critical effects of interacting with biased AI and its ability to impact public discourse and political conduct, but also highlights potential techniques for mitigating these risks in the future.</pre>
</div>


<script>
function toggleBibtexbias_llm(parameter) {
    var x= document.getElementById('abias_llm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractbias_llm(parameter) {
    var x= document.getElementById('bbias_llm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li></ol> -->
<!-- </div> -->

<!-- In the News -->

<div class="jumbotron">
    <h3 id="journal-articles">In the News</h3>
  
    <style>
      .bibliography {
        font-size: 1.1rem;
        line-height: 1.6;
        padding-left: 20px;
      }
      .bibliography li {
        margin-bottom: 10px;
      }
      .bibliography a {
        text-decoration: none;
      }
      .bibliography a .source {
        color: white;          /* text before colon */
        font-weight: 600;
      }
      .bibliography a .title {
        color: #00cc66;        /* green text after colon */
      }
      .bibliography a:hover .title {
        text-decoration: underline; /* underline only the green part */
      }
    </style>
  
    <ol class="bibliography">
      <li>
        <a href="https://www.kuow.org/stories/making-political-decisions-with-the-help-of-voter-guides-pamphlets-and-chatgpt">
          <span class="source">NPR Soundside</span>: 
          <span class="title">Making political decisions with the help of voter guides, pamphlets and... ChatGPT?</span>
        </a>
      </li>
      <li>
        <a href="https://mynorthwest.com/kiro/watch/ai-and-politics/0198cdb1-2b48-8008-c62b-a9aa0af7df0b">
          <span class="source">Seattle Morning News</span>: 
          <span class="title">AI and Politics</span>
        </a>
      </li>
      <li>
        <a href="https://www.geekwire.com/2025/ai-chatbots-sway-political-opinions-but-education-could-offer-protection-uw-study-finds/">
          <span class="source">GeekWire</span>: 
          <span class="title">UW study: Politically persuasive AI chatbots offer potential benefits — and worrying influence</span>
        </a>
      </li>
      <li>
        <a href="https://www.washington.edu/news/2025/08/06/biased-ai-chatbots-swayed-peoples-political-views/">
          <span class="source">UW News</span>: 
          <span class="title">With just a few messages, biased AI chatbots swayed people’s political views</span>
        </a>
      </li>
    </ol>
  </div>
  
  



<!-- CONFERENCE -->
<div class="jumbotron">
  <h3 id="conference-proceedings">Conference Proceedings</h3>
  <ol class="bibliography"><li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>
<div class="text-justify"><span id="rums"> <b>Fisher, J.</b>, Neville, J., &amp; Young Park, C. <u><i>Beyond Similarity for Personalization: User Memory Selection via Response-Utility Optimization</i> </u>. <i>In Review </i>.</span></div>
<!-- <a href="/papers/vlm_2025.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a> -->
<button class="btn btn-danger btm-sm" onclick="toggleBibtexrums()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractrums()">ABSTRACT</button>

<div id="arums" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{fisher_rums",
  title = {Beyond Similarity for Personalization: User Memory Selection via Response-Utility Optimization},
  author = {Jillian Fisher, Jennifer Neville, Chan Young Park}
  year = {2025},
  note = {},
  file = {}
}
</pre>
</div>

<div id="brums" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>A common approach to personalization in large language models (LLMs) is to incorporate a subset of the user memory into the prompt at inference time to guide the model's generation. Existing methods to select these subsets primarily rely on similarity between user memory items and input queries, ignoring how features actually affect the model's predictive distribution. We propose \textbf{R}esponse-\textbf{U}tility optimization for \textbf{M}emory \textbf{S}election (RUMS), a novel user memory selection method, inspired by Bayesian Optimal Experimental Design, that directly quantifies how much each memory item reduces uncertainty in the model's response distribution. RUMS measures mutual information between a subset of user memory and the model outputs to identify items that sharpen predictions beyond semantic similarity. Even more, RUMS, by design, automatically selects if personalization is beneficial at all. We demonstrate that this information-theoretic foundation enables more principled user memory selection that aligns more closely with human selection compared to state-of-the-art methods, and models $400 \times$ larger. Additionally, we show that memory items selected using RUMS result in better response quality compared to existing approaches, while having up to 95\% reduction in computational cost. </pre>
</div>


<script>
function toggleBibtexrums(parameter) {
    var x= document.getElementById('arums);
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractrums(parameter) {
    var x= document.getElementById('brums');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>
<div class="text-justify"><span id="friendly_vlm"> <b>Fisher, J.*</b>, Young Park, C.*, Memmel, M., Khullar, D., Yun, A., Gupta, A.,  &amp; Choi, Y. <u><i>Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning </i> </u>. <i>EMNLP 2025 </i>. <a href="https://arxiv.org/html/2507.08224v1">https://arxiv.org/html/2507.08224v1</a></span></div>
<a href="/papers/vlm_2025.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexfriendlyvlm()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractfriendlyvlm()">ABSTRACT</button>

<div id="afriendly_vlm" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{fisher_vlm_friendly",
  title = {Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning},
  author = {Chan Young Park, Jillian Fisher, Marius Memmel, Dipika Khullar, Andy Yun, Abhishek Gupta, Yejin Choi}
  year = {2025},
  note = {},
  file = {vlm_2025.pdf}
}
</pre>
</div>

<div id="bfriendly_vlm" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Large language models (LLMs) have shown promise in robotic procedural planning, yet their human-centric reasoning often omits the low-level, grounded details needed for robotic execution. Vision-language models (VLMs) offer a path toward more perceptually grounded plans, but current methods either rely on expensive, large-scale models or are constrained to narrow simulation settings. We introduce SelfReVision, a lightweight and scalable self-improvement framework for vision-language procedural planning. SelfReVision enables small VLMs to iteratively critique, revise, and verify their own plans-without external supervision or teacher models-drawing inspiration from chain-of-thought prompting and self-instruct paradigms. Through this self-distillation loop, models generate higher-quality, execution-ready plans that can be used both at inference and for continued fine-tuning. Using models varying from 3B to 72B, our results show that SelfReVision not only boosts performance over weak base VLMs but also outperforms models 100X the size, yielding improved control in downstream embodied tasks.</pre>
</div>


<script>
function toggleBibtexfriendlyvlm(parameter) {
    var x= document.getElementById('afriendly_vlm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractfriendlyvlm(parameter) {
    var x= document.getElementById('bfriendly_vlm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>

<div class="text-justify"><span id="pn"> <b>Fisher, J.</b>, Appel, R., Young Park, C., Potter, Y.,Jiang, L., Sorensen, T., Feng, S., Tsvetkov, Y., Roberts, M., Pan, J., Song, D.,  &amp; Choi, Y. <u><i>Political Neutrality in AI is Impossible- But Here is How to Approximate it </i> </u>. <i>ICML 2025 (Oral)</i>. <a href="https://arxiv.org/abs/2503.05728">https://arxiv.org/abs/2503.05728</a></span></div>
<a href="/papers/political_neutrality_2025.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexpn()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractpn()">ABSTRACT</button>
<a href="https://icml.cc/virtual/2025/oral/40158" target="_blank"><button class="btn btn-success btn-sm">VIDEO</button></a>

<div id="apn" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{fisher_political_neutrality",
  title = {Political Neutrality in AI is Impossible- But Here is How to Approximate it},
  author = {Jillian Fisher, Ruth E. Appel, Chan Young Park, Yujin Potter, Liwei Jiang, Taylor Sorensen, Shangbin Feng, Yulia Tsvetkov, Margaret E. Roberts, Jennifer Pan, Dawn Song, Yejin Choi},
  year = {2025},
  note = {},
  file = {political_neutrality_2025.pdf}
}
</pre>
</div>

<div id="bpn" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>AI systems often exhibit political bias, influencing users’ opinions and decision-making. While political neutrality—defined as the absence of bias—is often seen as an ideal solution for fairness and safety, this position paper argues that true political neutrality is neither feasible nor universally desirable due to its subjective nature and the biases inherent in AI training data, algorithms, and user interactions. However, inspired by Joseph Raz’s philosophical insight that “neutrality [...] can be a matter of degree” (Raz, 1986), we argue that striving for some neutrality remains essential for promoting balanced AI interactions and mitigating user manipulation. Therefore, we use the term “approximation” of political neutrality to shift the focus from unattainable absolutes to achievable, practical proxies. We propose eight techniques for approximating neutrality across three levels of conceptualizing AI, examining their trade-offs and implementation strategies. In addition, we explore two concrete applications of these approximations to illustrate their practicality. Finally, we assess our framework on current large language models (LLMs) at the output level, providing a demonstration of how it can be evaluated. This work seeks to advance nuanced discussions of political neutrality in AI and promote the development of responsible, aligned language models.</pre>
</div>


<script>
function toggleBibtexpn(parameter) {
    var x= document.getElementById('apn');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractpn(parameter) {
    var x= document.getElementById('bpn');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>

<div class="text-justify"><span id="bias_llm"> <b>Fisher, J.</b>, Feng, S., Aron, R., Richardson, T., Choi Y., Fisher, D., Pan J., Tsvetkov, Y.,  &amp; Reinecke, K. <u><i>Biased AI can Influence Political Decision-Making</i></u>. <i>ACL 2025 (Oral)</i>. <a href="https://arxiv.org/abs/2410.06415">arxiv.org/abs/2410.06415</a>.</span></div>
<a href="/papers/bias_llm.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexbias_llm()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractbias_llm()">ABSTRACT</button>

<div id="abias_llm" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{bias_llm",
  title = {Biased AI can Influence Political Decision-Making},
  author = {Jillian Fisher, Shangbin Feng, Robert Aron, Thomas Richardson, Yejin Choi, Daniel W. Fisher, Jennifer Pan, Yulia Tsvetkov, Katharina Reinecke},
  year = {2023},
  note = {https://arxiv.org/abs/2410.06415},
  file = {bias_llm.pdf}
}
</pre>
</div>

<div id="bbias_llm" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>As modern AI models become integral to everyday tasks, concerns about their inherent biases and their potential impact on human decision-making have emerged. While bias in models are well-documented, less is known about how these biases influence human decisions. This paper presents two interactive experiments investigating the effects of partisan bias in AI language models on political decision-making. Participants interacted freely with either a biased liberal, conservative, or unbiased control model while completing political decision-making tasks. We found that participants exposed to politically biased models were significantly more likely to adopt opinions and make decisions aligning with the AI’s bias, regardless of their personal political partisanship. However, we also discovered that prior knowledge about AI could lessen the impact of the bias, highlighting the possible importance of AI education for robust bias mitigation. Our findings not only highlight the critical effects of interacting with biased AI and its ability to impact public discourse and political conduct, but also highlights potential techniques for mitigating these risks in the future.</pre>
</div>


<script>
function toggleBibtexbias_llm(parameter) {
    var x= document.getElementById('abias_llm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractbias_llm(parameter) {
    var x= document.getElementById('bbias_llm');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>

<div class="text-justify"><span id="infoguided"> Ravichander A., <b>Fisher J.</b>, Sorensen T., Lu X., Lin Y., Antoniak M., Mireshghallah N., Bhagavatula C., &amp; Choi Y.<u><i>Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models</i></u>.<i> NAACL (2025)</i>. <a href="https://arxiv.org/abs/2503.12072">https://arxiv.org/abs/2503.12072</a>.</span></div>
<a href="/papers/guided_info.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexinfoguided()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractinfoguided()">ABSTRACT</button>

<div id="ainfoguided" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{infoguided,
    title={Information-Guided Identification of Training Data Imprint in (Proprietary) Large Language Models}, 
    author={Abhilasha Ravichander and Jillian Fisher and Taylor Sorensen and Ximing Lu and Yuchen Lin and Maria Antoniak and Niloofar Mireshghallah and Chandra Bhagavatula and Yejin Choi},
    journal = {NAACL}
  year = {2025},
  note = {https://arxiv.org/pdf/2503.12072},
  file = {guided_info.pdf}
}
</pre>
</div>


<div id="binfoguided" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>High-quality training data has proven crucial for developing performant large language models (LLMs). However, commercial LLM providers disclose few, if any, details about the data used for training. This lack of transparency creates multiple challenges: it limits external oversight and inspection of LLMs for issues such as copyright infringement, it undermines the agency of data authors, and it hinders scientific research on critical issues such as data contamination and data selection. How can we re- cover what training data is known to LLMs? In this work we demonstrate a new method to identify training data known to proprietary LLMs like GPT-4 without requiring any access to model weights or token probabilities, by using information-guided probes. Our work builds on a key observation: text passages with high surprisal are good search material for memorization probes. By evaluating a model’s ability to successfully reconstruct high-surprisal tokens in text, we can identify a surprising number of texts memorized by LLMs.</pre>
</div>

<script>
function toggleBibtexinfoguided(parameter) {
    var x= document.getElementById('ainfoguided');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractinfoguided(parameter) {
    var x= document.getElementById('binfoguided');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>

<div class="text-justify"><span id="styleremix"> <b>Fisher, J.</b>, Hallinan, S., Lu, X., Gordon, M., Harchaoui, Z.,  &amp; Choi, Y. <u><i>StyleRemix: Interpertable Authorship Obfuscation via Distillation and Perturbation of Style Elements</i></u>. <i>EMNLP 2024</i>. <a href="http://www.arxiv.org/abs/2408.15666">http://www.arxiv.org/abs/2408.15666</a>.</span></div>
<a href="/papers/styleremix.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexstyleremix()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractsyleremix()">ABSTRACT</button>

<div id="astyleremix" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{styleremix,
  title = {StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements},
  author = {Jillian Fisher, Skyler Hallinan, Ximing Lu, Mitchell Gordon, Zaid Harchaoui, Yejin Choi},
  journal = {EMNLP}
  year = {2024},
  note = {http://www.arxiv.org/abs/2408.15666},
  file = {styleremix.pdf}
}
</pre>
</div>


<div id="bstyleremix" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Authorship obfuscation, rewriting a text to intentionally obscure the identity of the author, is an important but challenging task. Current methods using large language models (LLMs) lack interpretability and controllability, often ignoring author-specific stylistic features, resulting in less robust performance overall.
    To address this, we develop StyleRemix, an adaptive and interpretable obfuscation method that perturbs specific, fine-grained style elements of the original input text. StyleRemix uses pre-trained Low Rank Adaptation (LoRA) modules to rewrite an input specifically along various stylistic axes (e.g., formality and length) while maintaining low computational cost. StyleRemix outperforms state-of-the-art baselines and much larger LLMs in a variety of domains as assessed by both automatic and human evaluation.
    Additionally, we release AuthorMix, a large set of 30K high-quality, long-form texts from a diverse set of 14 authors and 4 domains, and DiSC, a parallel corpus of 1,500 texts spanning seven style axes in 16 unique directions.</pre>
</div>

<script>
function toggleBibtexstyleremix(parameter) {
    var x= document.getElementById('astyleremix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractsyleremix(parameter) {
    var x= document.getElementById('bstyleremix');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>

<div class="text-justify"><span id="modular_pluralism">Feng, S., Sorensen, Taylor., Liu, Y., <b>Fisher, J.</b>, Young Park, C., Choi, Y., &amp; Tsvetkov, Y. <u><i>Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration</i></u>. <i>EMNLP 2024</i>. <a href="https://arxiv.org/abs/2406.15951">https://arxiv.org/abs/2406.15951</a>.</span></div>
<a href="/papers/modular_pluralism.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>

<button class="btn btn-danger btm-sm" onclick="toggleBibtexmodular_pluralism()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractmodular_pluralism()">ABSTRACT</button>

<div id="amodular_pluralism" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{modular_pluralism,
  title = {Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration},
  author = {Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, Yulia Tsvetkov},
  journal = {EMNLP},
  year = {2024},
  note = {https://arxiv.org/abs/2406.15951},
  file = {modular_pluralism.pdf}
}
</pre>
</div>


<div id="bmodular_pluralism" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>While existing alignment paradigms have been integral in developing large language models (LLMs), LLMs often learn an averaged human preference and struggle to model diverse preferences across cultures, demographics, and communities. We propose Modular Pluralism, a modular framework based on multi-LLM collaboration for pluralistic alignment: it "plugs into" a base LLM a pool of smaller but specialized community LMs, where models collaborate in distinct modes to flexibility support three modes of pluralism: Overton, steerable, and distributional. Modular Pluralism is uniquely compatible with black-box LLMs and offers the modular control of adding new community LMs for previously underrepresented communities. We evaluate Modular Pluralism with six tasks and four datasets featuring questions/instructions with value-laden and perspective-informed responses. Extensive experiments demonstrate that Modular Pluralism advances the three pluralism objectives across six black-box and open-source LLMs. Further analysis reveals that LLMs are generally faithful to the inputs from smaller community LLMs, allowing seamless patching by adding a new community LM to better cover previously underrepresented communities.</pre>
</div>

<script>
function toggleBibtexmodular_pluralism(parameter) {
    var x= document.getElementById('amodular_pluralism');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractmodular_pluralism(parameter) {
    var x= document.getElementById('bmodular_pluralism');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="pluralism_roadmap">Sorensen, T., Moore, Jared., <b>Fisher, J.</b>, Gordon, M., Mireshghallah, N., Rytting, C., Ye, A., Jiang, L., Lu, X., Dziri, N., Althoff, T., &amp; Choi, Y. <u><i>A Roadmap to Pluralistic Alignment</i></u>. <i>ICML 2024</i>. <a href="https://arxiv.org/abs/2402.05070">https://arxiv.org/abs/2402.05070</a>.</i></span></div>
<a href="/papers/roadmap_to_pluralistic_alignment.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>

<button class="btn btn-danger btm-sm" onclick="toggleBibtexpluralism_roadmap()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractpluralism_roadmap()">ABSTRACT</button>

<div id="apluralism_roadmap" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{pluralism_roadmap,
  title = {A Roadmap to Pluralistic Alignment},
  author = {Sorensen, Taylor and Moore, Jared and Fisher, Jillian and Gordon, Mitchell and Mireshghallah, Niloofar and Rytting, Christopher Michael and Ye, Andre and Jiang, Liwei and Lu, Ximing and Dziri, Nouha and Althoff, Tim and Choi, Yejin},
  journal = {ICML},
  year = {2024},
  note = {https://arxiv.org/abs/2402.05070},
  file = {roadmap_to_pluralistic_alignment.pdf}
}
</pre>
</div>


<div id="bpluralism_roadmap" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>With increased power and prevalence of AI systems, it is ever more critical that AI systems are designed to serve all, i.e., people with diverse values and perspectives. However, aligning models to serve pluralistic human values remains an open research question. In this piece, we propose a roadmap to pluralistic alignment, specifically using language models as a test bed. We identify and formalize three possible ways to define and operationalize pluralism in AI systems: 1) Overton pluralistic models that present a spectrum of reasonable responses; 2) Steerably pluralistic models that can steer to reflect certain perspectives; and 3) Distributionally pluralistic models that are well-calibrated to a given population in distribution. We also propose and formalize three possible classes of pluralistic benchmarks: 1) Multi-objective benchmarks, 2) Trade-off steerable benchmarks, which incentivize models to steer to arbitrary trade-offs, and 3) Jury-pluralistic benchmarks which explicitly model diverse human ratings. We use this framework to argue that current alignment techniques may be fundamentally limited for pluralistic AI; indeed, we highlight empirical evidence, both from our own experiments and from other work, that standard alignment procedures might reduce distributional pluralism in models, motivating the need for further research on pluralistic alignment.</pre>
</div>

<script>
function toggleBibtexpluralism_roadmap(parameter) {
    var x= document.getElementById('apluralism_roadmap');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractpluralism_roadmap(parameter) {
    var x= document.getElementById('bpluralism_roadmap');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="author_obf"><b>Fisher, J.</b>, Lu, X., Jung, J., Jiang, L., &amp; Choi, Y. <u><i>JAMDEC: Unsupervised Authorship Obfuscation using Contrained Decoding over Small Language Models</i></u>. <i>NAACL 2024 (Oral)</i>. <a href="https://arxiv.org/abs/2402.08761">https://arxiv.org/abs/2402.08761</a>.</span></div>
<a href="/papers/JAMDEC.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexauthor_obf()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractauthor_obf()">ABSTRACT</button>

<div id="aauthor_obf" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{author_obf,
  author = {Fisher, Jillian and Lu, Ximing and Jung, Jaehun and Jiang, Liwei and Choi, Yejin},
  title = {JAMDEC: Unsupervised Authorship Obfuscation using Contrained Decoding over Small Language Models},
  journal = {NAACL},
  note = {https://arxiv.org/abs/2402.08761},
  file = {JAMDEC.pdf},
  year = {2024}
}
</pre>
</div>

<div id="bauthor_obf" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and  privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency. We introduce MASQUERADE Decoding, a  user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM’s APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement. The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility. Experimental results demonstrate that our approach based on GPT2-XL outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against GPT3 175B, a propriety model that is two orders of magnitudes larger. </pre>
</div>

<script>
function toggleBibtexauthor_obf(parameter) {
    var x= document.getElementById('aauthor_obf');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractauthor_obf(parameter) {
    var x= document.getElementById('bauthor_obf');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>



<div class="text-justify"><span id="impossible_distillation">Jung, J., West, P., Jiang, L., Brahman, F., Lu, X., <b>Fisher, J.</b>, Sorensen, T., &amp; Choi, Y. <u> <i>Impossible Distillation: from Low-Quality Model to High-Quality Dataset &amp; Model for Summarization and Paraphrasing</i></u>. <i>NAACL 2023</i>. <a href="https://arxiv.org/abs/2305.16635">https://arxiv.org/abs/2305.16635</a>.</span></div>
<a href="/papers/impossible_distill.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibteximpossible_distillation()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractimpossible_distillation()">ABSTRACT</button>

<div id="aimpossible_distillation" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{impossible_distillation,
  title = {Impossible Distillation: from Low-Quality Model to High-Quality Dataset &amp; Model for Summarization and Paraphrasing},
  author = {Jung, Jaehun and West, Peter and Jiang, Liwei and Brahman, Faeze and Lu, Ximing and Fisher, Jillian and Sorensen, Taylor and Choi, Yejin},
  year = {2023},
  note = {https://arxiv.org/abs/2305.16635},
  file = {impossible_distill.pdf}
}
</pre>
</div>


<div id="bimpossible_distillation" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>It is commonly perceived that the strongest language models (LMs) rely on a combination of massive scale, instruction data, and human feedback to perform specialized tasks – e.g. summarization and paraphrasing, without supervision. In this paper, we propose that language models can learn to summarize and paraphrase sentences, with none of these 3 factors. We present Impossible Distillation, a framework that distills a task-specific dataset directly from an off-the-shelf LM, even when it is impossible for the LM itself to reliably solve the task. By training a student model on the generated dataset and amplifying its capability through self-distillation, our method yields a high-quality model and dataset from a low-quality teacher model, without the need for scale or supervision. Using Impossible Distillation, we are able to distill an order of magnitude smaller model (with only 770M parameters) that outperforms 175B parameter GPT-3, in both quality and controllability, as confirmed by automatic and human evaluations. Furthermore, as a useful byproduct of our approach, we obtain DIMSUM+, a high-quality dataset with 3.4M sentence summaries and paraphrases. Our analyses show that this dataset, as a purely LM-generated corpus, is more diverse and more effective for generalization to unseen domains than all human-authored datasets – including Gigaword with 4M samples.</pre>
</div>

<script>
function toggleBibteximpossible_distillation(parameter) {
    var x= document.getElementById('aimpossible_distillation');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractimpossible_distillation(parameter) {
    var x= document.getElementById('bimpossible_distillation');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>

<!-- </script></li></ol>
</div> -->


<!-- </script></li></ol>
</div> -->


<div class="text-justify"><span id="generative_paradox">West, P., Lu, X., Dziri, N., Brahman, F., Li, L., Hwang, J. D., Jiang, L., <b>Fisher, J.</b>, Ravichander, A., Chandu, K., Newman, B., Koh, P. W., Ettinger, A., &amp; Choi, Y. <u><i>The Generative AI Paradox: "What It Can Create, It May Not Understand"</i></u>. <i>ICLR 2024</i>. <a href="https://arxiv.org/abs/2311.00059">https://arxiv.org/abs/2311.00059</a>.</span></div>
<a href="/papers/generation_paradox.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexgenerative_paradox()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractgenerative_paradox()">ABSTRACT</button>

<div id="agenerative_paradox" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{generative_paradox,
  author = {West, Peter and Lu, Ximing and Dziri, Nouha and Brahman, Faeze and Li, Linjie and Hwang, Jena D. and Jiang, Liwei and Fisher, Jillian and Ravichander, Abhilasha and Chandu, Khyathi and Newman, Benjamin and Koh, Pang Wei and Ettinger, Allyson and Choi, Yejin},
  title = {The Generative AI Paradox: "What It Can Create, It May Not Understand"},
  year = {2024},
  note = {https://arxiv.org/abs/2311.000595},
  file = {generation_paradox.pdf},
  month = dec,
  address = {Vienna},
  publisher = {International Conference on Learning Representations (ICLR)}
}
</pre>
</div>


<div id="bgenerative_paradox" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artifi- cial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we rec- oncile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today’s generative models relative to intel- ligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert- like outputs, acquire generative capabilities that are not contingent upon—and can therefore exceed—their ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities. Our results show that although models can outperform humans in generation, they consistently fall short of human capabili- ties in measures of understanding, showing weaker correlation between generation and understanding performance, and more brittleness to adversarial inputs. Our findings support the hypothesis that models’ generative capability may not be con- tingent upon understanding capability, and call for caution in interpreting artificial intelligence by analogy to human intelligence.</pre>
</div>

<script>
function toggleBibtexgenerative_paradox(parameter) {
    var x= document.getElementById('agenerative_paradox');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractgenerative_paradox(parameter) {
    var x= document.getElementById('bgenerative_paradox');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="ipa">Lu, X., Brahman, F., West, P., Jung, J., Chandu, K., Ravichander, A., Ammanabrolu, P., Jiang, L., Ramnath, S., Dziri, N., <b>Fisher, J.</b>, Lin, B., Hallinan, S., Qin, L., Ren, X., Welleck, S., &amp; Choi, Y. <u><i>Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning</i></u>. <i>EMNLP 2023</i>. <a href="https://aclanthology.org/2023.emnlp-main.424">https://aclanthology.org/2023.emnlp-main.424</a>.</span></div>
<a href="/papers/ipa.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
<a href="http://doi.org/10.18653/v1/2023.emnlp-main.424" target="_blank"><button class="btn btn-primary btm-sm">DOI</button></a>
<button class="btn btn-danger btm-sm" onclick="toggleBibtexipa()">BIB</button>
<button class="btn btn-warning btm-sm" onclick="toggleAbstractipa()">ABSTRACT</button>


<div id="aipa" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{ipa,
  title = {Inference-Time Policy Adapters ({IPA}): Tailoring Extreme-Scale {LM}s without Fine-tuning},
  author = {Lu, Ximing and Brahman, Faeze and West, Peter and Jung, Jaehun and Chandu, Khyathi and Ravichander, Abhilasha and Ammanabrolu, Prithviraj and Jiang, Liwei and Ramnath, Sahana and Dziri, Nouha and Fisher, Jillian and Lin, Bill and Hallinan, Skyler and Qin, Lianhui and Ren, Xiang and Welleck, Sean and Choi, Yejin},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  month = dec,
  year = {2023},
  address = {Singapore},
  publisher = {EMNLP},
  note = {https://aclanthology.org/2023.emnlp-main.424},
  doi = {10.18653/v1/2023.emnlp-main.424},
  pages = {6863--6883},
  file = {ipa.pdf}
}
</pre>
</div>


<div id="bipa" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.</pre>
</div>

<script>
function toggleBibtexipa(parameter) {
    var x= document.getElementById('aipa');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractipa(parameter) {
    var x= document.getElementById('bipa');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px:
}
pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="influence_theory"><b>Fisher, J.</b>, Liu, L., Pillutla, K., Choi, Y., &amp; Harchaoui, Z. <u><i>Statistical and Computational Guarantees for Influence Diagnostics</i></u>. <i>AISTAT 2023</i>. <a href="https://proceedings.mlr.press/v206/fisher23a/fisher23a.pdf">https://proceedings.mlr.press/v206/fisher23a/fisher23a.pdf</a>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="influence_theory">Fisher, J., Liu, L., Pillutla, K., Choi, Y., &amp; Harchaoui, Z. (2023, April). <i>Statistical and Computational Guarantees for Influence Diagnostics</i>. https://proceedings.mlr.press/v206/fisher23a/fisher23a.pdf.</span></div> -->





<a href="/papers/influence_theory.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>

<a href="http://doi.org/10.48550/arXiv.2212.04014" target="_blank"><button class="btn btn-primary btm-sm">DOI</button></a>

<button class="btn btn-danger btm-sm" onclick="toggleBibtexinfluence_theory()">BIB</button>

<button class="btn btn-warning btm-sm" onclick="toggleAbstractinfluence_theory()">ABSTRACT</button>

<div id="ainfluence_theory" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{influence_theory,
  title = {Statistical and Computational Guarantees for Influence Diagnostics},
  author = {Fisher, Jillian and Liu, Lang and Pillutla, Krishna and Choi, Yejin and Harchaoui, Zaid},
  year = {2023},
  month = apr,
  address = {Valencia},
  publisher = {Artificial Intelligence and Statistics (AISTATS)},
  doi = {10.48550/arXiv.2212.04014},
  note = {https://proceedings.mlr.press/v206/fisher23a/fisher23a.pdf},
  primaryclass = {stat.ML},
  file = {influence_theory.pdf}
}
</pre>
</div>


<div id="binfluence_theory" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Influence diagnostics such as influence functions and approximate maximum influence perturba- tions are popular in machine learning and in AI domain applications. Influence diagnostics are powerful statistical tools to identify influential datapoints or subsets of datapoints. We establish finite-sample statistical bounds, as well as com- putational complexity bounds, for influence func- tions and approximate maximum influence per- turbations using efficient inverse-Hessian-vector product implementations. We illustrate our results with generalized linear models and large attention based models on synthetic and real data.</pre>
</div>

<script>
function toggleBibtexinfluence_theory(parameter) {
    var x= document.getElementById('ainfluence_theory');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractinfluence_theory(parameter) {
    var x= document.getElementById('binfluence_theory');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script></li></ol>
</div>

<!-- JOURNALS -->

<div class="jumbotron">
    <h3 id="journal-articles">Journal Articles</h3>
    <ol class="bibliography"><li><style>
  .btn{
      margin-bottom:5px;
      padding-top:0px;
      padding-bottom:0px;
      padding-left:15px;
      padding-right:15px;
      height:20px:
  }
  pre{
      white-space: pre-wrap;  
      white-space: -moz-pre-wrap; 
      white-space: -pre-wrap; 
      white-space: -o-pre-wrap; 
      word-wrap: break-word; 
      width:100%; overflow-x:auto;
  }
  </style>
  
  
  <div class="text-justify"><span id="smoking_cessation">O.Baird, S., Rinck, M., Rosenfield, D., Davis, M. L., <b>Fisher, J.</b>, Becker, E. S., Powers, M. B., &amp; Smits, J. A. J. <u><i> Reducing approach bias to achieve smoking cessation: A pilot randomized placebo-controlled trial.</i></u> <i>Cognitive Therapy and Research (2017)</i>, <i>4</i>(41).</span></div>
  <a href="/papers/smoking_cessation.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>
  <button class="btn btn-danger btm-sm" onclick="toggleBibtexsmoking_cessation()">BIB</button>
  <button class="btn btn-warning btm-sm" onclick="toggleAbstractsmoking_cessation()">ABSTRACT</button>
  
  <div id="asmoking_cessation" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
  <pre>@article{smoking_cessation,
    author = {O.Baird, Scarlett and Rinck, Mike and Rosenfield, David and Davis, Michelle L. and Fisher, Jillian and Becker, Eni S. and Powers, Mark B. and Smits, Jasper A. J.},
    journal = {Cognitive Therapy and Research},
    number = {41},
    title = {Reducing approach bias to achieve smoking cessation: A pilot randomized placebo-controlled trial.},
    volume = {4},
    year = {2017},
    file = {smoking_cessation.pdf}
  }
  </pre>
  </div>
  
  
  <div id="bsmoking_cessation" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
  <pre>This study aimed to provide a preliminary test of the efficacy of a brief cognitive bias modification program for reducing approach bias in adult smokers motivated to quit. Participants were 52 smokers who were randomly assigned to four sessions of approach bias modification training (AAT) or sham training. Participants were asked to make a self-guided quit attempt upon completion of the final training session. Approach bias was assessed at baseline and at the end of each session, and days abstinent was assessed 1-week following the quit attempt. Individuals assigned to the AAT training condition evidenced significantly greater reductions in approach bias relative to those in the sham condition (p &lt; .001). Baseline approach bias did not moderate the between-group effect (ps &gt; 0.41); however, higher levels of approach bias at baseline were associated with greater approach bias reduction over time irrespective of condition (p &lt; .001). Consistent with hypothesis, the reduction in approach bias during the intervention period was significantly related to the number of days abstinent following the quit attempt (p = .033). The present study extends recent work in alcohol use disorders by showing that approach bias reduction, in this case for smoking-related stimuli, may also facilitate smoking cessation. Clinical and research implications are discussed.</pre>
  </div>
  
  <script>
  function toggleBibtexsmoking_cessation(parameter) {
      var x= document.getElementById('asmoking_cessation');
      if (x.style.display === 'none') {
          x.style.display = 'block';
      } else {
          x.style.display = 'none';
      }
  }
  function toggleAbstractsmoking_cessation(parameter) {
      var x= document.getElementById('bsmoking_cessation');
      if (x.style.display === 'none') {
          x.style.display = 'block';
      } else {
          x.style.display = 'none';
      }
  }
  </script></li></ol>
  </div>

Dictionary:
<ul>
  <li> <b>NAACL</b>: North American Chapter of the Association of Computational Linguistics</li>
  <li> <b>EMNLP</b>: Empirical Methods in Natural Language Processing</li>
  <li> <b>ICML</b>: International Conference on Machine Learning</li>
  <li> <b>ICLR</b>: International Conference on Learning Representation</li>
  <li> <b>AISTATS</b>: Artificial Intelligence and Statistics</li>

</div>

      </div>
    </div>


<!-- 
<div class="jumbotron">
  <h4>News</h4>

  
  
  <h5><a href="/allnews.html">... see all News</a></h5>

</div> -->

  </div>

      </div>
    </div>

    <br/>
<section id="footer">
<div class="container-footer">
  <div class="panel-footer">
	  <div class="row">
		<div class="col-sm-4">
		    <h5>About</h5>	
            <p>Jillian Fisher<br/> Statistics PhD Student<br/> University of Washington
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Contact</h5>	
            <p><a href="mailto:jrfish@uw.edu" target="_blank"><i class="fa fa-envelope fa-1x"></i> Contact Jillian via email</a> <br/> <a href="https://github.com/sbryngelson/academic-website-template"><i class="fa fa-github fa-1x"></i> This site was adapted from others and is open source</a>
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Other Pages</h5>	
            <p><a href="https://scholar.google.com/citations?user=Gnk0E_QAAAAJ&hl=en" target="_blank"><i class="fa fa-github fa-1x"></i> Google Scholar</a> <br/> <a href="https://www.semanticscholar.org/author/Jillian-R.-Fisher/33772445"><i class="fa fa-github fa-1x"></i> Semantic Scholar </a> <br/> <a href="https://www.linkedin.com/in/jillianrosefisher/" target="_blank"><i class="fa fa-github fa-1x"></i> LinkedIn</a> <br/>
</p>
		</div>
	  </div>

      <center><p>&copy 2024 Jillian Fisher </p></center>
	</div>
  </div>
</div>

<script src="/assets/javascript/bootstrap/jquery.min.js"></script>
<script src="/assets/javascript/bootstrap/bootstrap.bundle.min.js"></script>


  </body>

</html>
